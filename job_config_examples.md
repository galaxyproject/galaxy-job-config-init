# Job Configuration Examples

This page provides examples of Galaxy job configuration files generated by the `galaxy-job-config-init` command.

## Without Shared TPV Database ()

### Local Runner and No Containers



```
galaxy-job-config-init  
```

-or-

```
planemo job_config_init  
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4


handling:
  assign:
    - db-skip-locked


execution:
  default: local
  environments:
    local:
      runner: local
      tmp_dir: true

tools:
  - class: local
    environment: local
```


### Local Runner and Docker



```
galaxy-job-config-init --docker 
```

-or-

```
planemo job_config_init --docker 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4


handling:
  assign:
    - db-skip-locked


execution:
  default: local
  environments:
    local:
      runner: local
      tmp_dir: true
      docker_enabled: true
      docker_cmd: docker
      docker_volumes: $defaults

tools:
  - class: local
    environment: local
```


### Local Runner and Singularity



```
galaxy-job-config-init --singularity 
```

-or-

```
planemo job_config_init --singularity 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4


handling:
  assign:
    - db-skip-locked


execution:
  default: local
  environments:
    local:
      runner: local
      tmp_dir: true
      singularity_enabled: true
      singularity_cmd: singularity
      singularity_volumes: $defaults
      ## May need to setup addition environment variables to get singularity working based on examples @
      ## https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
      # env:
      # - name: LC_ALL
      #   value: C
      # - name: APPTAINER_CACHEDIR
      #   value: /tmp/singularity
      # - name: APPTAINER_TMPDIR
      #   value: /tmp

tools:
  - class: local
    environment: local
```


### SLURM Runner and No Containers



```
galaxy-job-config-init --runner slurm 
```

-or-

```
planemo job_config_init --runner slurm 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    # modify below to specify a specific drmaa shared library for slurm
    # drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1


handling:
  assign:
    - db-skip-locked


execution:
  default: slurm
  environments:
    local:
      runner: local
      tmp_dir: true

    # Information on connecting Galaxy to SLURM can be found at:
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
    slurm:
      runner: slurm
      native_specification: "" 
      tmp_dir: true
tools:
  - class: local
    environment: local
```


### DRMAA Runner and No Containers



```
galaxy-job-config-init --runner drmaa 
```

-or-

```
planemo job_config_init --runner drmaa 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  drmaa:
    load: galaxy.jobs.runners.drmaa:DRMAAJobRunner
    # modify below to specify a specific drmaa shared library
    # drmaa_library_path: /sge/lib/libdrmaa.so


handling:
  assign:
    - db-skip-locked


execution:
  default: drmaa
  environments:
    local:
      runner: local
      tmp_dir: true

    drmaa:
      runner: drmaa
      native_specification: "" 
      tmp_dir: true
tools:
  - class: local
    environment: local
```


### Condor Runner and No Containers



```
galaxy-job-config-init --runner condor 
```

-or-

```
planemo job_config_init --runner condor 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  condor:
    load: galaxy.jobs.runners.condor:CondorJobRunner


handling:
  assign:
    - db-skip-locked


execution:
  default: condor
  environments:
    local:
      runner: local
      tmp_dir: true

    condor:
      runner: condor
      # universe: "vanilla"
      # Additional/override query ClassAd params can be specified here
      tmp_dir: true
tools:
  - class: local
    environment: local
```


### K8S Runner and Docker



```
galaxy-job-config-init --runner k8s --docker 
```

-or-

```
planemo job_config_init --runner k8s --docker 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  k8s:
    load: galaxy.jobs.runners.kubernetes:KubernetesJobRunner


handling:
  assign:
    - db-skip-locked


execution:
  default: k8s
  environments:
    local:
      runner: local
      tmp_dir: true
      docker_enabled: true
      docker_cmd: docker
      docker_volumes: $defaults

    k8s:
      runner: k8s
      tmp_dir: true
      docker_enabled: true
      docker_cmd: docker
      docker_volumes: $defaults
tools:
  - class: local
    environment: local
```


### SLURM Runner and Singularity



```
galaxy-job-config-init --runner slurm  --singularity
```

-or-

```
planemo job_config_init --runner slurm  --singularity
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    # modify below to specify a specific drmaa shared library for slurm
    # drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1


handling:
  assign:
    - db-skip-locked


execution:
  default: slurm
  environments:
    local:
      runner: local
      tmp_dir: true
      singularity_enabled: true
      singularity_cmd: singularity
      singularity_volumes: $defaults
      ## May need to setup addition environment variables to get singularity working based on examples @
      ## https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
      # env:
      # - name: LC_ALL
      #   value: C
      # - name: APPTAINER_CACHEDIR
      #   value: /tmp/singularity
      # - name: APPTAINER_TMPDIR
      #   value: /tmp

    # Information on connecting Galaxy to SLURM can be found at:
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
    slurm:
      runner: slurm
      native_specification: "" 
      tmp_dir: true
      singularity_enabled: true
      singularity_cmd: singularity
      singularity_volumes: $defaults
      ## May need to setup addition environment variables to get singularity working based on examples @
      ## https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
      # env:
      # - name: LC_ALL
      #   value: C
      # - name: APPTAINER_CACHEDIR
      #   value: /tmp/singularity
      # - name: APPTAINER_TMPDIR
      #   value: /tmp
tools:
  - class: local
    environment: local
```


## With TPV in 25.0+

### Local Runner and No Containers



```
galaxy-job-config-init --tpv 
```

-or-

```
planemo job_config_init --tpv 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4


handling:
  assign:
    - db-skip-locked


execution:
  default: tpv
  environments:
    local:
      runner: local
      tmp_dir: true

    # Information on configuring TPV and leveraging the shared TPV Galaxy database for tools can be found at @
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/job-destinations/tutorial.html
    tpv:
      runner: dynamic_tpv
      tpv_configs:
      - https://gxy.io/tpv/db.yml
      - destinations:
          tpvdb_local:
            tmp_dir: true
tools:
  - class: local
    environment: local
```


### Local Runner and Docker



```
galaxy-job-config-init --docker --tpv 
```

-or-

```
planemo job_config_init --docker --tpv 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4


handling:
  assign:
    - db-skip-locked


execution:
  default: tpv
  environments:
    local:
      runner: local
      tmp_dir: true
      docker_enabled: true
      docker_cmd: docker
      docker_volumes: $defaults

    # Information on configuring TPV and leveraging the shared TPV Galaxy database for tools can be found at @
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/job-destinations/tutorial.html
    tpv:
      runner: dynamic_tpv
      tpv_configs:
      - https://gxy.io/tpv/db.yml
      - destinations:
          tpvdb_local:
            tmp_dir: true
            docker_enabled: true
            docker_cmd: docker
            docker_volumes: $defaults
tools:
  - class: local
    environment: local
```


### Local Runner and Singularity



```
galaxy-job-config-init --singularity --tpv 
```

-or-

```
planemo job_config_init --singularity --tpv 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4


handling:
  assign:
    - db-skip-locked


execution:
  default: tpv
  environments:
    local:
      runner: local
      tmp_dir: true
      singularity_enabled: true
      singularity_cmd: singularity
      singularity_volumes: $defaults
      ## May need to setup addition environment variables to get singularity working based on examples @
      ## https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
      # env:
      # - name: LC_ALL
      #   value: C
      # - name: APPTAINER_CACHEDIR
      #   value: /tmp/singularity
      # - name: APPTAINER_TMPDIR
      #   value: /tmp

    # Information on configuring TPV and leveraging the shared TPV Galaxy database for tools can be found at @
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/job-destinations/tutorial.html
    tpv:
      runner: dynamic_tpv
      tpv_configs:
      - https://gxy.io/tpv/db.yml
      - destinations:
          tpvdb_local:
            tmp_dir: true
            singularity_enabled: true
            singularity_cmd: singularity
            singularity_volumes: $defaults
            ## May need to setup addition environment variables to get singularity working based on examples @
            ## https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
            # env:
            # - name: LC_ALL
            #   value: C
            # - name: APPTAINER_CACHEDIR
            #   value: /tmp/singularity
            # - name: APPTAINER_TMPDIR
            #   value: /tmp
tools:
  - class: local
    environment: local
```


### SLURM Runner and No Containers



```
galaxy-job-config-init --runner slurm --tpv 
```

-or-

```
planemo job_config_init --runner slurm --tpv 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    # modify below to specify a specific drmaa shared library for slurm
    # drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1


handling:
  assign:
    - db-skip-locked


execution:
  default: tpv
  environments:
    local:
      runner: local
      tmp_dir: true

    # Information on connecting Galaxy to SLURM can be found at:
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
    slurm:
      runner: slurm
      native_specification: "" 
      tmp_dir: true

    # Information on configuring TPV and leveraging the shared TPV Galaxy database for tools can be found at @
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/job-destinations/tutorial.html
    tpv:
      runner: dynamic_tpv
      tpv_configs:
      - https://gxy.io/tpv/db.yml
      - destinations:
          tpvdb_slurm:
            tmp_dir: true
tools:
  - class: local
    environment: local
```


### DRMAA Runner and No Containers



```
galaxy-job-config-init --runner drmaa --tpv 
```

-or-

```
planemo job_config_init --runner drmaa --tpv 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  drmaa:
    load: galaxy.jobs.runners.drmaa:DRMAAJobRunner
    # modify below to specify a specific drmaa shared library
    # drmaa_library_path: /sge/lib/libdrmaa.so


handling:
  assign:
    - db-skip-locked


execution:
  default: tpv
  environments:
    local:
      runner: local
      tmp_dir: true

    drmaa:
      runner: drmaa
      native_specification: "" 
      tmp_dir: true

    # Information on configuring TPV and leveraging the shared TPV Galaxy database for tools can be found at @
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/job-destinations/tutorial.html
    tpv:
      runner: dynamic_tpv
      tpv_configs:
      - https://gxy.io/tpv/db.yml
      - destinations:
          tpvdb_drmaa:
            runner: drmaa
            params:
              # adapt {cores} and {mem} to your DRM here - likely using native_specification
              native_specification: ""

            tmp_dir: true
tools:
  - class: local
    environment: local
```


### Condor Runner and No Containers



```
galaxy-job-config-init --runner condor --tpv 
```

-or-

```
planemo job_config_init --runner condor --tpv 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  condor:
    load: galaxy.jobs.runners.condor:CondorJobRunner


handling:
  assign:
    - db-skip-locked


execution:
  default: tpv
  environments:
    local:
      runner: local
      tmp_dir: true

    condor:
      runner: condor
      # universe: "vanilla"
      # Additional/override query ClassAd params can be specified here
      tmp_dir: true

    # Information on configuring TPV and leveraging the shared TPV Galaxy database for tools can be found at @
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/job-destinations/tutorial.html
    tpv:
      runner: dynamic_tpv
      tpv_configs:
      - https://gxy.io/tpv/db.yml
      - destinations:
          tpvdb_condor:
            runner: condor
            params:
              request_cpus: "{cores}"
              # universe: "vanilla"
              # Additional/override query ClassAd params can be specified here

            tmp_dir: true
tools:
  - class: local
    environment: local
```


### K8S Runner and Docker



```
galaxy-job-config-init --runner k8s --docker --tpv 
```

-or-

```
planemo job_config_init --runner k8s --docker --tpv 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  k8s:
    load: galaxy.jobs.runners.kubernetes:KubernetesJobRunner


handling:
  assign:
    - db-skip-locked


execution:
  default: tpv
  environments:
    local:
      runner: local
      tmp_dir: true
      docker_enabled: true
      docker_cmd: docker
      docker_volumes: $defaults

    k8s:
      runner: k8s
      tmp_dir: true
      docker_enabled: true
      docker_cmd: docker
      docker_volumes: $defaults

    # Information on configuring TPV and leveraging the shared TPV Galaxy database for tools can be found at @
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/job-destinations/tutorial.html
    tpv:
      runner: dynamic_tpv
      tpv_configs:
      - https://gxy.io/tpv/db.yml
      - destinations:
          tpvdb_k8s:
            runner: k8s
            tmp_dir: true
            docker_enabled: true
            docker_cmd: docker
            docker_volumes: $defaults
tools:
  - class: local
    environment: local
```


## With Legacy TPV in 24.2

### Local Runner and No Containers



```
galaxy-job-config-init --tpv --galaxy-version 24.2 
```

-or-

```
planemo job_config_init --tpv --galaxy_version 24.2 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4


handling:
  assign:
    - db-skip-locked


execution:
  default: tpv
  environments:
    local:
      runner: local
      tmp_dir: true

    # Information on configuring TPV and leveraging the shared TPV Galaxy database for tools can be found at @
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/job-destinations/tutorial.html
    tpv:
      runner: dynamic
      type: python
      function: map_tool_to_destination
      rules_module: tpv.rules
      tpv_config_files:
      - https://gxy.io/tpv/db.yml
      - /Users/mvandenb/src/galaxy-job-config-init/tpv.yml  # TODO: create this file and setup a TPV destination
##  tpv.yml should contain something like:
# global:
#   default_inherits: default
# destinations:
#   tpvdb_local:
#     tmp_dir: true
tools:
  - class: local
    environment: local
```


### SLURM Runner and Singularity



```
galaxy-job-config-init --runner slurm --tpv --singularity  --galaxy-version 24.2 
```

-or-

```
planemo job_config_init --runner slurm --tpv --singularity  --galaxy_version 24.2 
```

generate the follow job configuration YAML file:

```yaml
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    # modify the number of threads working on local jobs here
    # workers: 4

  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    # modify below to specify a specific drmaa shared library for slurm
    # drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1


handling:
  assign:
    - db-skip-locked


execution:
  default: tpv
  environments:
    local:
      runner: local
      tmp_dir: true
      singularity_enabled: true
      singularity_cmd: singularity
      singularity_volumes: $defaults
      ## May need to setup addition environment variables to get singularity working based on examples @
      ## https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
      # env:
      # - name: LC_ALL
      #   value: C
      # - name: APPTAINER_CACHEDIR
      #   value: /tmp/singularity
      # - name: APPTAINER_TMPDIR
      #   value: /tmp

    # Information on connecting Galaxy to SLURM can be found at:
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
    slurm:
      runner: slurm
      native_specification: "" 
      tmp_dir: true
      singularity_enabled: true
      singularity_cmd: singularity
      singularity_volumes: $defaults
      ## May need to setup addition environment variables to get singularity working based on examples @
      ## https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
      # env:
      # - name: LC_ALL
      #   value: C
      # - name: APPTAINER_CACHEDIR
      #   value: /tmp/singularity
      # - name: APPTAINER_TMPDIR
      #   value: /tmp

    # Information on configuring TPV and leveraging the shared TPV Galaxy database for tools can be found at @
    # https://training.galaxyproject.org/training-material/topics/admin/tutorials/job-destinations/tutorial.html
    tpv:
      runner: dynamic
      type: python
      function: map_tool_to_destination
      rules_module: tpv.rules
      tpv_config_files:
      - https://gxy.io/tpv/db.yml
      - /Users/mvandenb/src/galaxy-job-config-init/tpv.yml  # TODO: create this file and setup a TPV destination
##  tpv.yml should contain something like:
# global:
#   default_inherits: default
# destinations:
#   tpvdb_slurm:
#     tmp_dir: true
#     singularity_enabled: true
#     singularity_cmd: singularity
#     singularity_volumes: $defaults
#     ## May need to setup addition environment variables to get singularity working based on examples @
#     ## https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html
#     # env:
#     # - name: LC_ALL
#     #   value: C
#     # - name: APPTAINER_CACHEDIR
#     #   value: /tmp/singularity
#     # - name: APPTAINER_TMPDIR
#     #   value: /tmp
tools:
  - class: local
    environment: local
```